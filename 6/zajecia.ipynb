{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **GenAI** - Podstawy budowy LLM/DDPM, ich zastosowanie, problemy etyczno-prawne\n",
    "\n",
    "Organizator: Koło naukowe BioMedicalAI  \n",
    "![biomedical.svg](biomedical.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Budowa LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Toenization](./htokenization.png)  \n",
    "*LLM  https://docs.mistral.ai/guides/tokenization/*\n",
    "1) Tokenizer  \n",
    "Kodowanie tekstu na tokeny czyli ciąg liczb określających dany zestaw znaków/słów/bajtów.\n",
    "\n",
    "2) Embedding  \n",
    "Przekształcanie tokenów na wektory określające koncepty/role w zależności od pozycji tokenu w sekwencji.\n",
    "\n",
    "3) Bloki transformerowe  \n",
    "Przetwarzają sekwencję w kontekście wyucoznego zadania. Np. predykcja kolejnego tokenu, predykcja tokenu zastępczego (FIM - fill in the middle), NER - rozpoznawanie jednostek nazwanych, detekcja sentymentu, summaryzacja, tłumaczenie tekstu, question answering\n",
    "\n",
    "4) Deembedding  \n",
    "Przekształcanie embeddingów na sekwencję tokenów.\n",
    "\n",
    "5) Decoding  \n",
    "Dekodowanie tokenów na sekwencję znaków/słów/bajtów."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Embedding - czyli jak zakodować pozycję tokenu\n",
    "\n",
    "![Embeddings](./embeddings.png)  \n",
    "*Klatka z filmu wizualizującego działanie pozycyjnego embeddingu https://www.youtube.com/watch?v=1biZfFLPRSY*\n",
    "\n",
    "Tokeny na początku są przetwarzane przez embedder, który przekształca token na wektor osadzenia opisujący koncept/rolę tokenu.\n",
    "Dla każdego wymiaru $j$ wektora osadzenia $d_E$ są budowane funkcje $sin_{ij}$, $cos_{ij}$ (gdzie $i$ to pozycja tokenu) o różnej częstotliwości.\n",
    "Następnie do każdego embeddingu tokenu są sumowane wektory osadzeń pozycji. Pozwala to przekształcenie wektora w wymiarach, aby wpłynąć na działanie modeli upstreamowych.\n",
    "\n",
    "\n",
    "Artykuł nt. positionl Embeddingu https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jak działa blok transformerowy\n",
    "Wszystko dzięki mechanizmowi atencji/uwagi.  \n",
    "$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$  \n",
    "\n",
    "\n",
    "![Attention visualization](./hqkv.png)  \n",
    "*Klatka z filmu 3Brown1Blue opisujący działanie mechanizmu atencji. https://www.youtube.com/watch?v=eMlx5fFNoYc*\n",
    "\n",
    "\n",
    "![Attention diagram](./mha.webp)  \n",
    "*Diagram z artykułu https://towardsdatascience.com/transformers-in-action-attention-is-all-you-need-ac10338a023a*  \n",
    "Mechanizm atencji ten pozwala na przekształcenie wektorów sekwencji w celu zakodowania danego kontekstu oraz zadania.\n",
    "Wektor sekwencji (E) mnożony jest przez macierze W_q, W_k, W_v dając odpowiednio Q, K, V. Następnie Q - query i K - keys są mnożone dając macierz jak dany klucz odpowiada danemu zapytaniu , dzielony przez prierwiastek z wymiarowości macierzy kluczy (w celu zmniejszenia wariancji) i  wykonywana jest funkcja softmax w celu normalizacji wartości do zakresu 0-1. Wynik ten jest mnożony przez macierz V która daje wynik $\\nabla_E$, który następnie dodawany jest do wektora E aby zakodować dane wyliczone w danej wartswie.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Narzędzia dla LLMów/wykorzystujące LLM warte uwagi  \n",
    "* Copilot - narzędzie do pracy z kodem (pytania do kodu/dokumentacji), predykcja kodu\n",
    "* TabbyML - open-source alternatywa dla Copilota\n",
    "* Ollama - open-source alternatywa api OpenAI\n",
    "* ChatGPT - generacja tekstu na podstawie zapytania (promptu) - chatbot\n",
    "* Perplexity - wyszukiwarka danych z interfejsem ChatGPT\n",
    "* Claude - chatbot \n",
    "\n",
    "## Powszechnie używane rodziny modeli LLM\n",
    "* ChatGPT + GPT* - rodzina modeli rozwijanych przez OpenAI\n",
    "* Gemini + Gemma - rodzina modeli rozwijane przez Google\n",
    "* Llama - rodzina modeli roziwjana przez Meta\n",
    "* Mistral/Codestral - rodzina modeli rozwijanych przez Mistral AI\n",
    "* Phi - rodzina modeli rozijanych przez Microsoft (mały rozmiar przy utrzymaniu wysokich wyników)\n",
    "* Grok - model rozwijany przez xAI\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "# Dziękuję za udział 🎉\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
