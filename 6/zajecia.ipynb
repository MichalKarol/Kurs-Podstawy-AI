{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **GenAI** - Podstawy budowy LLM/DDPM, ich zastosowanie, problemy etyczno-prawne\n",
    "\n",
    "Organizator: Koo naukowe BioMedicalAI  \n",
    "![biomedical.svg](biomedical.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Budowa LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Toenization](./htokenization.png)  \n",
    "*LLM  https://docs.mistral.ai/guides/tokenization/*\n",
    "1) Tokenizer  \n",
    "Kodowanie tekstu na tokeny czyli cig liczb okrelajcych dany zestaw znak贸w/s贸w/bajt贸w.\n",
    "\n",
    "2) Embedding  \n",
    "Przeksztacanie token贸w na wektory okrelajce koncepty/role w zale偶noci od pozycji tokenu w sekwencji.\n",
    "\n",
    "3) Bloki transformerowe  \n",
    "Przetwarzaj sekwencj w kontekcie wyucoznego zadania. Np. predykcja kolejnego tokenu, predykcja tokenu zastpczego (FIM - fill in the middle), NER - rozpoznawanie jednostek nazwanych, detekcja sentymentu, summaryzacja, tumaczenie tekstu, question answering\n",
    "\n",
    "4) Deembedding  \n",
    "Przeksztacanie embedding贸w na sekwencj token贸w.\n",
    "\n",
    "5) Decoding  \n",
    "Dekodowanie token贸w na sekwencj znak贸w/s贸w/bajt贸w."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Embedding - czyli jak zakodowa pozycj tokenu\n",
    "\n",
    "![Embeddings](./embeddings.png)  \n",
    "*Klatka z filmu wizualizujcego dziaanie pozycyjnego embeddingu https://www.youtube.com/watch?v=1biZfFLPRSY*\n",
    "\n",
    "Tokeny na pocztku s przetwarzane przez embedder, kt贸ry przeksztaca token na wektor osadzenia opisujcy koncept/rol tokenu.\n",
    "Dla ka偶dego wymiaru $j$ wektora osadzenia $d_E$ s budowane funkcje $sin_{ij}$, $cos_{ij}$ (gdzie $i$ to pozycja tokenu) o r贸偶nej czstotliwoci.\n",
    "Nastpnie do ka偶dego embeddingu tokenu s sumowane wektory osadze pozycji. Pozwala to przeksztacenie wektora w wymiarach, aby wpyn na dziaanie modeli upstreamowych.\n",
    "\n",
    "\n",
    "Artyku nt. positionl Embeddingu https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jak dziaa blok transformerowy\n",
    "Wszystko dziki mechanizmowi atencji/uwagi.  \n",
    "$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$  \n",
    "\n",
    "\n",
    "![Attention visualization](./hqkv.png)  \n",
    "*Klatka z filmu 3Brown1Blue opisujcy dziaanie mechanizmu atencji. https://www.youtube.com/watch?v=eMlx5fFNoYc*\n",
    "\n",
    "\n",
    "![Attention diagram](./mha.webp)  \n",
    "*Diagram z artykuu https://towardsdatascience.com/transformers-in-action-attention-is-all-you-need-ac10338a023a*  \n",
    "Mechanizm atencji ten pozwala na przeksztacenie wektor贸w sekwencji w celu zakodowania danego kontekstu oraz zadania.\n",
    "Wektor sekwencji (E) mno偶ony jest przez macierze W_q, W_k, W_v dajc odpowiednio Q, K, V. Nastpnie Q - query i K - keys s mno偶one dajc macierz jak dany klucz odpowiada danemu zapytaniu , dzielony przez prierwiastek z wymiarowoci macierzy kluczy (w celu zmniejszenia wariancji) i  wykonywana jest funkcja softmax w celu normalizacji wartoci do zakresu 0-1. Wynik ten jest mno偶ony przez macierz V kt贸ra daje wynik $\\nabla_E$, kt贸ry nastpnie dodawany jest do wektora E aby zakodowa dane wyliczone w danej wartswie.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Narzdzia dla LLM贸w/wykorzystujce LLM warte uwagi  \n",
    "* Copilot - narzdzie do pracy z kodem (pytania do kodu/dokumentacji), predykcja kodu\n",
    "* TabbyML - open-source alternatywa dla Copilota\n",
    "* Ollama - open-source alternatywa api OpenAI\n",
    "* ChatGPT - generacja tekstu na podstawie zapytania (promptu) - chatbot\n",
    "* Perplexity - wyszukiwarka danych z interfejsem ChatGPT\n",
    "* Claude - chatbot \n",
    "\n",
    "## Powszechnie u偶ywane rodziny modeli LLM\n",
    "* ChatGPT + GPT* - rodzina modeli rozwijanych przez OpenAI\n",
    "* Gemini + Gemma - rodzina modeli rozwijane przez Google\n",
    "* Llama - rodzina modeli roziwjana przez Meta\n",
    "* Mistral/Codestral - rodzina modeli rozwijanych przez Mistral AI\n",
    "* Phi - rodzina modeli rozijanych przez Microsoft (may rozmiar przy utrzymaniu wysokich wynik贸w)\n",
    "* Grok - model rozwijany przez xAI\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "# Dzikuj za udzia \n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
