{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Computer Vision** - Podstawy działania i przykłady zastosowania sieci neuronowych do klasyfikacji obrazów i wykrywania obiektów\n",
    "\n",
    "Organizator: Koło naukowe BioMedicalAI  \n",
    "![biomedical.svg](biomedical.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Czym jest konwolucja (okno przesuwne)?\n",
    "\n",
    "Konwolucja jest operacją matematyczną przekształcenia macierzowego danych wejściowych.\n",
    "Macierz tego przekształcenia nazywamy filtrem/kernelem.\n",
    "\n",
    "![\"Przykład konwolucji\"](./Convolution_arithmetic_-_Padding_strides.gif)  \n",
    "*Przykłąd konwolucji*\n",
    "*Źródło: Animation of a variation of the convolution operation. Blue maps are inputs, and cyan maps are outputs. From Vincent Dumoulin, Francesco Visin - A guide to convolution arithmetic for deep learning[1]*\n",
    "\n",
    "[A guide to convolution arithmetic for deep learning https://arxiv.org/pdf/1603.07285](https://arxiv.org/pdf/1603.07285)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (15,15)\n",
    "\n",
    "image = cv.cvtColor(cv.imread(\"cv.png\"), cv.COLOR_BGR2GRAY)\n",
    "plt.imshow(image, cmap=\"grey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prosta konwolucja\n",
    "identity = np.array([\n",
    "    [0, 0, 0],\n",
    "    [0, 1, 0],\n",
    "    [0, 0, 0],\n",
    "], dtype=float)\n",
    "# Sobel\n",
    "vertical_edge_detection = np.array([\n",
    "    [-1, 0, 1],\n",
    "    [-2, 0, 2],\n",
    "    [-1, 0, 1],\n",
    "], dtype=float)\n",
    "\n",
    "horizontal_edge_detection = np.array([\n",
    "    [-1, -2, -1],\n",
    "    [0, 0, 0],\n",
    "    [1, 2, 1],\n",
    "], dtype=float)\n",
    "kernel = vertical_edge_detection\n",
    "\n",
    "new_image = np.zeros_like(image, dtype=float)\n",
    "for i in range(image.shape[0] - 2):\n",
    "    for j in range(image.shape[1] - 2):\n",
    "        new_image[i, j] = np.sum(kernel * image[i:i+3, j:j+3])\n",
    "plt.imshow(new_image, cmap=\"grey\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Czym jest stride, padding i dylatacja?\n",
    "Stride  - krok o ile przesuwa się okno\n",
    "Padding - wypełnienie pozwalające zachować wielkość obrazu\n",
    "Dylatacja - przestrzeń pomiędzy elementami macierzy\n",
    "\n",
    "![\"Przykład konwolucji z dylatacją i paddingiem\"](./image.png)  \n",
    "*Przykład konwolucji z dylatacją i paddingiem*  \n",
    "*Żródło: Animation of a variation of the convolution operation. Blue maps are inputs, and cyan maps are outputs. From Vincent Dumoulin, Francesco Visin - A guide to convolution arithmetic for deep learning*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling\n",
    "Pooling pozwala na processing danych i wyodrębnienie z nich cech.\n",
    "Wyodrebniamy Min/Avg/Max pooling.\n",
    "\n",
    "![max_pool.png](./max_pool.png)  \n",
    "*Max pooling. Źródło: https://computersciencewiki.org/index.php?title=Max-pooling_/_Pooling*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasyfikacja MNIST uzywajać LeNet5\n",
    "**https://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "import lightning as L\n",
    "import torchmetrics as TM\n",
    "import os\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "train = datasets.MNIST('../data', train=True, download=True, transform=(transforms.ToTensor()))\n",
    "test = datasets.MNIST('../data', train=False, download=True, transform=(transforms.ToTensor()))\n",
    "\n",
    "class LightningClassification(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, kernel_size=5),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.layer_2 = nn.Sequential(\n",
    "            nn.Conv2d(6, 16, kernel_size=5),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.fc5 = nn.Linear(256, 120)\n",
    "        self.fc6 = nn.Linear(120, 84)\n",
    "        self.fc7 = nn.Linear(84, 10)\n",
    "            \n",
    "    def model(self, x):\n",
    "        x = self.layer_1(x)\n",
    "        x = self.layer_2(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc5(x)\n",
    "        x = torch.nn.functional.sigmoid(x)\n",
    "        x = self.fc6(x)\n",
    "        x = torch.nn.functional.sigmoid(x)\n",
    "        x = self.fc7(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_pred = self.model(x)\n",
    "        loss = nn.functional.cross_entropy(y_pred, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        x, y = batch\n",
    "        y_pred = self.model(x)\n",
    "        self.log(\"val_accuracy\", TM.functional.accuracy(y_pred, y, task=\"multiclass\", num_classes=10))\n",
    "        self.log(\"val_precision\", TM.functional.precision(y_pred, y, task=\"multiclass\", num_classes=10), prog_bar=True)\n",
    "        self.log(\"val_matthews_corrcoef\", TM.functional.matthews_corrcoef(y_pred, y, task=\"multiclass\", num_classes=10), prog_bar=True)\n",
    "    \n",
    "    def test_step(self, batch):\n",
    "        x, y = batch\n",
    "        y_pred = self.model(x)\n",
    "        self.log(\"test_accuracy\", TM.functional.accuracy(y_pred, y, task=\"multiclass\", num_classes=10))\n",
    "        self.log(\"test_precision\", TM.functional.precision(y_pred, y, task=\"multiclass\", num_classes=10), prog_bar=True)\n",
    "        self.log(\"test_matthews_corrcoef\", TM.functional.matthews_corrcoef(y_pred, y, task=\"multiclass\", num_classes=10), prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "classifier = LightningClassification()\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train, batch_size=32, shuffle=True, num_workers=os.cpu_count() -1)\n",
    "test_dataloader = torch.utils.data.DataLoader(test, batch_size=32, num_workers=os.cpu_count() -1)\n",
    "\n",
    "trainer = L.Trainer(max_epochs=10)\n",
    "trainer.fit(model=classifier, train_dataloaders=train_dataloader, val_dataloaders=test_dataloader)\n",
    "trainer.test(classifier, test_dataloader)\n",
    "\n",
    "# Wizualizacja wyników klasyfikatora MNIST\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(test), size=(1,)).item()\n",
    "    img, label = test[sample_idx]\n",
    "    label_pred_vector = classifier.model(img[None, :])\n",
    "    label_pred = torch.argmax(label_pred_vector, axis=1)\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(f\"{label} / {label_pred.item()}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architektury\n",
    "\n",
    "### ResNet\n",
    "Block ResNet na znaczącą poprawę uczenia głebokich konwolucyjnych sieci neuronowych.  \n",
    "![image-2.png](./resblock.png)  \n",
    "[Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n",
    "\n",
    "### UNet\n",
    "Poprawa segmentacji poprzez wykorzystanie wyuczonej augmentacji danych wraz z kontekstem.  \n",
    "![image.png](./unet.png)  \n",
    "[U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597)\n",
    "\n",
    "\n",
    "### R-CNN (Regions with CNN feature)\n",
    "Rodzina architektur do wykrywania obiektów oraz segmentacji obrazów.  \n",
    "![image-3.png](./rcnn.png)  \n",
    "Składają się na nią:  \n",
    "[R-CNN](https://arxiv.org/abs/1311.2524v5)  \n",
    "[Fast R-CNN](https://arxiv.org/abs/1504.08083)  \n",
    "[Faster R-CNN](https://arxiv.org/abs/1506.01497)  \n",
    "[Mask R-CNN](https://arxiv.org/abs/1703.06870)\n",
    "\n",
    "### YOLO\n",
    "Rodzina architektur pozwalająca na detekcję obiektow real-time na niskiej klasy hardware.  \n",
    "![image-4.png](./yolo.png)  \n",
    "\n",
    "[You Only Look Once: Unified, Real-Time Object Detection](https://arxiv.org/abs/1506.02640)\n",
    "\n",
    "\n",
    "Zasoby:  \n",
    "[https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e](https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e)  \n",
    "[https://d2l.ai/chapter_computer-vision/rcnn.html](https://d2l.ai/chapter_computer-vision/rcnn.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metryki\n",
    "\n",
    "### Segmentacja\n",
    "Interception Over Union (IOU) - róznica między iloczynem zbiorów (ground truth i detection) oraz ich sumą\n",
    "![image.png](./iou.png)  \n",
    "*Intersection over Union (IoU) in Object Detection & Segmentation  https://learnopencv.com/intersection-over-union-iou-in-object-detection-and-segmentation/*\n",
    "\n",
    "Dice - $\\frac{2*Intersection}{Union} $\n",
    "\n",
    "### Detekcja obiektów\n",
    "Mean Average Precision (mAP) - metryka dająca informację nt. jakości detekcji obiektów dla wybranego IOU(np. 0.5). Na podstawie wyników detekcji dla obrazów przygotowuje się krzywą Precision-Recall dla klasy, pod którą pole to Average Precision. Następnie uśredniamy AP kla wszystkich klas aby uzyskać mAP.     \n",
    "![image-2.png](./precision-recall.png)  \n",
    "*Precision Recall curve graph. Image: Ren Jie Tan https://builtin.com/articles/mean-average-precision*\n",
    "\n",
    "\n",
    "### Generacja obrazów\n",
    "SSIM - Structural similarity index measure  \n",
    "Fréchet inception distance (FID) - różnica rozkładów pomiędzy aktywacjami Inception-v3 dla serii obrazów prawdziwycj i wygenerowanych\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wizualizacja filtrów\n",
    "\n",
    "### Aktywacja\n",
    "![image.png](./activations.png)  \n",
    "*How to visualize convolutional features in 40 lines of code https://towardsdatascience.com/how-to-visualize-convolutional-features-in-40-lines-of-code-70b7d87b0030*  \n",
    "\n",
    "### GradCam\n",
    "![image-2.png](./gradcam.png)\n",
    "*M, M.M., T. R, M., V, V.K. et al. Enhancing brain tumor detection in MRI images through explainable AI using Grad-CAM with Resnet 50. BMC Med Imaging 24, 107 (2024). https://doi.org/10.1186/s12880-024-01292-7*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wykorzystanie UNet w praktyce\n",
    "W ramach praktyki wysegmentujemy gruczoły znajdujące się w tkance w barwieniu H&E (hematoksyliną i eozyną).  \n",
    "Do tego zadania wykorzystamy dataset [GlaS MICCAI'2015](https://www.kaggle.com/datasets/sani84/glasmiccai2015-gland-segmentation)  \n",
    "[Link do ZIPa z datasetem](https://drive.google.com/file/d/1KwCeBziUnnYM0rXWLWXLxAGiN1M3TpGo/view?usp=drive_link)\n",
    "\n",
    "[Strona https://paperswithcode.com agregująca zarówno datasety oraz metody](https://paperswithcode.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as pp\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torchvision.transforms.functional import normalize\n",
    "\n",
    "# DataModule\n",
    "class GlaSDataModule():\n",
    "    class GlaSDataset(Dataset):\n",
    "        def __init__(self, dt, y, data_path):\n",
    "            self.dt = dt\n",
    "            self.y = y\n",
    "            self.data_path = data_path\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.dt)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            item = self.dt.iloc[idx]\n",
    "            image = torch.load(pp.join(self.data_path, \"tiles\", f\"{item[\"name\"]}.pth\"), weights_only=True)\n",
    "            mask = torch.load(pp.join(self.data_path, \"tiles\", f\"{item[\"name\"]}_anno.pth\"), weights_only=True)\n",
    "            return normalize(image, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), mask, self.y[idx]\n",
    "\n",
    "    def __init__(self, data_path, tile_size):\n",
    "        dataset = pd.read_csv(pp.join(data_path, \"Grade.csv\"))\n",
    "        self.dt_train = dataset[dataset[\"name\"].str.startswith(\"train\")]\n",
    "        self.dt_test = dataset[dataset[\"name\"].str.startswith(\"test\")]\n",
    "        if not pp.exists(pp.join(data_path, \"tiles\")):\n",
    "            os.mkdir(pp.join(data_path, \"tiles\"))\n",
    "\n",
    "        def convert_to_tiles(items):\n",
    "            results = []\n",
    "            for _, item in items.iterrows():\n",
    "                image = cv.imread(pp.join(data_path, f\"{item[\"name\"]}.bmp\"))\n",
    "                image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "                mask = cv.imread(pp.join(data_path, f\"{item[\"name\"]}_anno.bmp\"))\n",
    "                mask = mask[:, :, 0]\n",
    "                mask = np.where(mask > 0, 1, 0)\n",
    "                for i in range(0, int(np.ceil(image.shape[0]/tile_size))):\n",
    "                    for j in range(0, int(np.ceil(image.shape[1]/tile_size))):\n",
    "                        cut_image = image[i*tile_size:(i+1)*tile_size, j*tile_size:(j+1)*tile_size]\n",
    "                        if cut_image.shape[0] * cut_image.shape[1] < 0.3 * tile_size * tile_size:\n",
    "                            continue\n",
    "\n",
    "                        tile_image = np.zeros((tile_size, tile_size, 3))\n",
    "                        tile_image[:cut_image.shape[0], :cut_image.shape[1], :] = cut_image\n",
    "                        cut_mask = mask[i*tile_size:(i+1)*tile_size, j*tile_size:(j+1)*tile_size]\n",
    "                        tile_mask = np.zeros((tile_size, tile_size, 1))\n",
    "                        tile_mask[:cut_mask.shape[0], :cut_mask.shape[1], 0] = cut_mask\n",
    "                        torch.save(torch.FloatTensor(tile_image).permute(2, 0, 1) / 255, pp.join(data_path, \"tiles\", f\"{item[\"name\"]}_{i}_{j}.pth\"))\n",
    "                        torch.save(torch.IntTensor(tile_mask).permute(2, 0, 1), pp.join(data_path, \"tiles\", f\"{item[\"name\"]}_{i}_{j}_anno.pth\"))\n",
    "                        tile_item = item.to_dict()\n",
    "                        tile_item[\"name\"] = f\"{item[\"name\"]}_{i}_{j}\"\n",
    "                        results.append(tile_item)\n",
    "            return results\n",
    "\n",
    "        self.dt_train = pd.DataFrame(convert_to_tiles(self.dt_train))\n",
    "        self.dt_test = pd.DataFrame(convert_to_tiles(self.dt_test))\n",
    "\n",
    "        self.grade_encoder = OneHotEncoder()\n",
    "        self.train_y = self.grade_encoder.fit_transform(self.dt_train[\" grade (GlaS)\"].values.reshape(-1, 1)).toarray()\n",
    "        self.test_y = self.grade_encoder.transform(self.dt_test[\" grade (GlaS)\"].values.reshape(-1, 1)).toarray()\n",
    "        self.data_path = data_path\n",
    "\n",
    "    @property\n",
    "    def train(self):\n",
    "        return GlaSDataModule.GlaSDataset(self.dt_train, self.train_y, self.data_path)\n",
    "    \n",
    "    @property\n",
    "    def test(self):\n",
    "        return GlaSDataModule.GlaSDataset(self.dt_test, self.test_y, self.data_path)\n",
    "\n",
    "\n",
    "datamodule = GlaSDataModule(\"./warwick_qu_dataset_released_2016_07_08/Warwick QU Dataset (Released 2016_07_08)/\", 128)\n",
    "train_dataloader = DataLoader(datamodule.train, batch_size=32, shuffle=True, num_workers=os.cpu_count() -1)\n",
    "test_dataloader = DataLoader(datamodule.test, batch_size=32, num_workers=os.cpu_count() -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wizualizacja datasetu\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "images, masks, labels  = next(iter(test_dataloader))\n",
    "\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(images), size=(1,)).item()\n",
    "    img = images[sample_idx].permute(1, 2, 0).numpy()\n",
    "    mask = masks[sample_idx].permute(1, 2, 0).numpy()\n",
    "    figure.add_subplot(rows, cols * 2, i*2-1)\n",
    "    plt.title(datamodule.grade_encoder.inverse_transform(labels[sample_idx].numpy().reshape(1, -1))[0][0])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze())\n",
    "    figure.add_subplot(rows, cols * 2, i*2)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(mask.squeeze())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/milesial/Pytorch-UNet\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(input, target):\n",
    "    dims = (1, 2, 3)\n",
    "    intersection = torch.sum(input * target, dims)\n",
    "    cardinality = torch.sum(input + target, dims)\n",
    "\n",
    "    dice_score = 2.0 * intersection / (cardinality +  1e-6)\n",
    "    return torch.mean(1.0 - dice_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch import loggers\n",
    "\n",
    "class LightningUNet(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.n_classes = 1\n",
    "\n",
    "        self.inc = DoubleConv(3, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 1024)\n",
    "        self.up4 = Up(1024, 512)\n",
    "        self.up3 = Up(512, 256)\n",
    "        self.up2 = Up(256, 128)\n",
    "        self.up1 = Up(128, 64)\n",
    "        self.outc = OutConv(64, self.n_classes)\n",
    "            \n",
    "    def model(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = x5\n",
    "        x = self.up4(x, x4)\n",
    "        x = self.up3(x, x3)\n",
    "        x = self.up2(x, x2)\n",
    "        x = self.up1(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, masks, _ = batch\n",
    "        y_pred = self.model(images)\n",
    "        loss = nn.functional.binary_cross_entropy_with_logits(y_pred.squeeze(1), masks.squeeze(1).float())\n",
    "        loss += dice_loss(nn.functional.sigmoid(y_pred), masks)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, masks, _ = batch\n",
    "        y_pred = self.model(images)\n",
    "        self.log(\"test_dice\", TM.functional.dice(y_pred, masks), prog_bar=True)\n",
    "        self.log(\"test_iou\", TM.functional.jaccard_index(y_pred, masks, task=\"binary\"), prog_bar=True)\n",
    "        if batch_idx == 0:\n",
    "            self.logger.experiment.add_image(\"input\", images[0], self.current_epoch)\n",
    "            self.logger.experiment.add_image(\"mask\", torch.vstack([masks[0], masks[0], masks[0]]), self.current_epoch)\n",
    "            self.logger.experiment.add_image(\"pred\", torch.vstack([y_pred[0], y_pred[0], y_pred[0]]), self.current_epoch)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "segmenter = LightningUNet()\n",
    "\n",
    "tb_logger = loggers.TensorBoardLogger(save_dir=\"unet_logs\")\n",
    "trainer = L.Trainer(max_epochs=25, logger=tb_logger)\n",
    "trainer.fit(model=segmenter, train_dataloaders=train_dataloader, val_dataloaders=test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ankieta\n",
    "![\"Ankieta\"](./ankieta.png)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
